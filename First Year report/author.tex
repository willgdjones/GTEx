%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox]{svmult}

% choose options for [] as required from the list
% in the Reference Guide
\usepackage[margin=1.0in]{geometry}
\usepackage[Export]{adjustbox}
\usepackage{subcaption}
\usepackage{wrapfig}
\captionsetup{compatibility=false}
\usepackage{mathptmx}       % selects Times Roman as basic font
\usepackage{helvet}         % selects Helvetica as sans-serif font
\usepackage{courier}        % selects Courier as typewriter font
\usepackage{type1cm}        % activate if the above 3 fonts are
\usepackage{float}      % not available on your system
\usepackage{listings}
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{blindtext}
\graphicspath{ {/Users/fonz/Documents/Projects/GTEx/plotting} }
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom
\linespread{1.5}
% see the list of further useful packages
% in the Reference Guide

\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

\newcommand*{\figuretitle}[1]{%
    {\centering%   <--------  will only affect the title because of the grouping (by the
    \textbf{#1}%              braces before \centering and behind \medskip). If you remove
    \par\medskip}%            these braces the whole body of a {figure} env will be centered.
}

\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title*{Associating gene expression to neural network derived image features}
\author{William Jones (First year report)}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\institute{William Jones \at Wellcome Trust Sanger Institute, \email{wj2@sanger.ac.uk}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle

% Abstract - 200 words
%  Background - 1200 words
%  Aims - 200 words
%  Methods - 500 words
%  Results - 1000 words
%  Discussion - 900 words
%  Future work - 1000 words
%  Gantt chart outlining your future work plans
%  Tables - don’t use an excessive number
%  Figures - don’t use an excessive number
%  References


\abstract{200 words}
{Finding the genetic basis of biological traits is the central aim of genetics and much of computational biology. For effective and interpretable results, it is important that these traits to be both quantifiable and measurable. Recently, imaging studies have made available large datasets where samples are annotated with extensive genetic and transcriptomic information. Owing to large variation within individual images, and the commonly small sample size, it is difficult to extract well-defined features from images when investigating the genetic background of imaging phenotypes. Existing work has focused on hand-crafted features, however we tackle this challenge with a different approach. We exploit neural networks, known for their ability to extract high-level concepts features from images and use the Genotype Tissue Expression (GTEx) Project high-resolution histology images annotated with bulk gene expression and genotype data. We define a novel scheme to extract global features from an entire histology slide, and compare the number of statistically significant associations found from features at different patch-sizes. We find that much of this variation is due to technical factors, but that there also exists evidence for biologically driven variation.}

\section{Background 1200 words}

The general field to investigate the genetic basis of imaging phenotypes originated through the study of the brain in the 1990s \cite{imaging-as-a-tool} \cite{imaging-genetics} when scientists were searched for the genetic basis of psychopathology. They used using functional neuroimaging to find genes that were active in the brain. Even today, the entire field still broadly maintains this as its focus, indicated by abstract submission for the recent IIGC (International Imaging Genetics Conference), the vast majority of which consider only neuroimaging data and investigate the genetic basis of diseases such as Parkinson's and Alzheimers \cite{imaging-genetics-parkinsons}.

In an entirely different field, histopathology, imaging techniques have popularly used in recent years. Histopathology refers to the microscopic examination of tissue in order to study the manifestations of disease. These tissues samples are obtained through surgery, biopsy, or autopsy and then undergo the process of chemical fixation with formalin. After processing and screening, the tissue slices are stained with a combination of hemotoxylin and eosin \cite{hematoxylin-and-eosin-staining} (often abbreviated H\&E). The purpose of the hemotoxylin is to stain nuclei blue, and the purpose of the eosin is the stain the cytoplasm and the extracellular connective tissue matrix pink. This staining gives histopathology images their visual characteristic colours.

Following staining, histological slides are investigated under a microscopy by a pathologist to give a medical diagnosis. Histological slides are used routinely in the clinic to diagnose breast cancer along with numerous other diseases.

Imaging techniques have been used in Computer-assisted diagnosis (CAD) \cite{histopathological-image-analysis} since the 1990s. Since 2010, with the advent of whole slide digital scanners, tissue histopathology slides can now be stored in digital image form. From this point, it became possible to use image analysis and machine learning techniques to complement the opinion of radiologists in disease detection and prognosis prediction.

To what extent are the visual characteristics of biomedical images influenced by our DNA, and the transcript levels within our tissues? Pathologists are trained to identify visual characteristics in histological slides that are indicative of disease onset. It is known that disease onset triggers changes in bulk RNA expression data in tissues where the diseases present. \cite{gene-expression-parkinsons} Furthermore, in many cases that the onset of disease has a genetic component \cite{what-is-complex-about-complex-disorders}. 

Investigating the interplay between these data modalities intertwines with imaging genetics and histopathology. Owing to the absence such datasets, it has until been impossible to answer until recently.

The Genotype Tissue Expression (GTEx) project \cite{GTEx-project} is an example of a dataset that could be used to answer such a problem. This is a repository comprising genetics and bulk RNA expression data from multiple tissues and multiple healthy donors originating from the United States. Early papers studying this data aimed to characterise gene expression variation and quantify eQTLs across different human tissues. At the time of analysis, the repository v6, consists of 449 genotyped individuals, and bulk RNA expression from 44 tissue types. Not every tissue type is available from each donor. A median of 15 tissues is given per individual, and a median of 155 samples per tissue type.

Late last year, in November 2016 - high resolution, histology Whole Side Tissue Images (WSTI) were made available for 34 tissue types \cite{GTEx-histology}. Crucially, each of these samples had annotated donor genotype data, and also the corresponding bulk RNA expression signatures. As such, this resource now had the required level of annotation to study the genetics of histology images. 

There have already been recent publications that have already looked at integrating these images with corresponding genetic and transcriptomic data in order to understand pathological phenotypes.  McCall et al \cite{complex-sources-of-variation} aim to use image data to reconcile the problem of unknown quantities of cell types within a tissue contributing to the bulk RNA expression of a sample. Within this analysis, they manually annotate the extent of pneumocyte hyperplasia of 114 lung images on a 0-3 scale, and find correlations between these annotations and expression levels of highly variable gene clusters. Although interesting, these image features needed to be handcrafted, a labour intensive and expensive process.

The field of deep learning exploded in notoriety when in 2012 Krizhevsky et al \cite{image-net} popularized Convolutional Neural networks (CNNs) by emphatically winning the ILSVRC (ImageNet Large Scale Visual Recognition Challenge) with a top-5 test error rate of 15.3\% compared to a score of 26.2\% achieved by the second best entry. Since this point, they have become the most popular classification method in computer vision research. Part of the reason why they are so effective, is that they are able to automatically learn feature representations of the classes that are present in the input data. In 2014, Simonyan et al \cite{deep-inside-convolutional-networks} at the Visual Geometry Group in Oxford investigated the final layers of their ILSVRC2013 entry, VGG, and demonstrated that maximizing the activations of neurons deep within the network could generate features similar to the class labels that it was trained upon. The conclusion of this is that discriminatory features from the images of the input data could be learned through the process of training the neural network to classify object classes. From 2014 onwards, a new type of CNN architecture appeared - the Inception architecture \cite{network-in-network}, based on the ILSVRC2014 winning entry GoogLeNet. This allowed neural networks to become deeper through being efficient with the number of parameters and allowed for more complex representations of imaging data.

Indirectly learning representations via training convolutional networks is not the only way to generate interesting feature representations. Lee et al \cite{scalable-unsupervised-learning} train convolutional deep belief network on datasets of natural faces, and demonstrate that early layers in the network serve as edge detectors, while middle and late neurons can progressively define complex image features like the contours of eyes, noses, and mouths, with the deepest able to fully the capture an approximate concept of a face.

Images are useful but underutilised source of biological information. Thanks to new techniques, we can now quantify them in useful ways. This leads to questions about the genetic basis of image-based traits, which is underexplored. The GTEX project dataset is the first that allows answering these questions. In this work, we aim to combine the state-of-the-art in each of these fields in order to investigate the interplay between genetics, image analysis and histopathology through the use of cutting edge deep learning techniques.


\section{Aims 200 words}
In my first year, I aimed to build upon these recent methods from computer vision research in order to relate biomedical images to genetics. Using the GTEx image dataset, I aimed to:
\begin{enumerate}
 \item Classify tissue type from histology images using neural networks
 \item Define a latent image representation using the trained using the trained convolutional neural network as a feature extractor.
 \item Understand and interpret these latent factors, their drivers of variation and the relationship of these high level representations to genotype and expression datasets.
\end{enumerate}
\label{sec:2}

\section{Methods 500 words}



\subsection{Tissue Classification}

\subsubsection{Dataset}
We investigated the samples of 10 well-known tissue types with the aim to classify each class based on small square patches extracted from within the tissue boundary. The tissue we considered were: Artery - Tibial, Brain - Cerebellum, Breast - Mammary Tissue, Heart - Left Ventricle, Liver, Lung, Ovary, Pancreas, Stomach and Testis.

\subsubsection{Training data generation}

\begin{wrapfigure}{l}{0.6\textwidth}
    \figuretitle{Exracting Tissue patches}
    \includegraphics[width=1\linewidth]{/FeatureExploration/extracting_tissue_patches} 
    \caption{\textcolor{ao(english)}{A} We illustrate how the tissue is segmented into the tissue foreground and background by Gaussian blurring followed by Otsu thresholding. \textcolor{ao(english)}{B} We illustrate where patch centers are located within a tissue boundary. The dots represent patch centers that fit inside the tissue boundary. }
    \label{fig:extracting_tissue_patches}
\end{wrapfigure}


Much of the Whole Slide Tissues Image consisted of white empty space, with the area of tissue in question occupying a smaller area in the center. As such, we needed to define a patch sampling strategy that only extracted patches within the tissue boundary. In short, we needed to define the boundary between the tissue foreground and background, with the foreground consisting of the tissue area, and the background consisting of white-space.  To do this, we applied the following steps: We assign one-hot encoded tissue labels to the patches, and leave aside one third\% of the patches as a validation set.

We chose 6 different pixel widths. 128, 256, 512, 1024, 2048, and 4096. We chose these patch sizes because they represent the diversity of features at different resolutions. Within the patches with width 128 pixels we see individual cell nuclei and clusters of different cell types, whereas in large patch sizes we features characteristic of the entire image, for example colour or tissue shape (Figure \ref{patches-different-scales}). 

\begin{figure}[H]
\figuretitle{Patches at different scales}
    \centering
    \includegraphics[width=0.75\textwidth]{/FeatureExploration/patches_at_different_scales}
    \caption{Resolutions captured by patches of different sizes. For patch sizes at 4096 pixels, whole slide image features are captured such as tissue shape and colour, whereas as smaller resolutions of 128 pixels tissue texture and cell type composition are visible.}
    \label{patches-different-scales}
\end{figure}


\begin{enumerate}
\item We segment the tissue slice into foreground and background by grayscaling the image, using a Gaussian blur \cite{shapiro-computer-vision} with kernel (51,51), followed by Otsu thresholding \cite{otsu-method} (Figure \ref{fig:extracting_tissue_patches}).
\item Given a patch size, $s$, we find all patches of size $s$ that lie within the tissue boundary. 
\end{enumerate}

We sample 50 image patches within the tissue boundary from 100 images of 10 tissues, giving a total of 5000 patches per image class, with 50000 patches in total. 

\subsubsection{Neural network model}
We use Inceptionet-v3, a 220-layer Convolutional Neural Network with pre-trained weights to distinguish everyday objects in images. We follow the common practice of adjusting the network architecture in order to fine-tune the network and repurpose it for a different task \cite{fine-tuning-deep-convolutional-neural-networks}. To finetune this network, we add a GlobalAveragePooling layer \cite{network-in-network} followed by a Dense layer a neural network, and a final softmax layer with 10 classification neurons.

When varying the size of patches, we re-scale all patches to be 299x299 pixels. This means that if we classify a patch of size 4096, the size is drastically reduced to be 299x299. If the patch-size is 128x128, then we use bi-linear interpolation to resize the patch to be 299x299 pixels. 

\subsubsection{Training and Evaluation}
We fine-tuned our modified version of InceptioNet to classify square image patches into their originating tissue types. We use the categorical cross-entropy loss function with Stochastic Gradient Descent with learning rate 0.0001 and momentum = 0.9. We run the back-propagation algorithm to fine-tune the network in the following two steps:

We evaluate the performance of the trained network on a validation set
\begin{enumerate}
 \item Update the final-layer weights for 10 epochs.
 \item Update the InceptionNet-v3 layer weights for 30 epochs.
\end{enumerate}

We assess the performance of the classifier on the held-out validation set by reporting the percentage of correctly classified tissues.

\subsubsection{Tools}
We used Keras 2.0 \cite{keras} to build and train the neural networks. We use OpenSlide Python \cite{openslide}  version 1.1.1 to read in the whole slide images.

\subsection{Latent factors}

We choose Lung as the exemplary tissue in which to explore this method. We have a high number of images from this tissue, and the image slices tend to be large. Concretely, we generated image features for each Lung image, using individual lung patches via the following steps.

\begin{enumerate}
\item We pass every patch that lies within a tissue boundary through the raw and retrained InceptioNet networks, and at each patch-size, to obtain an image feature vector of length $1024$ for each patch.
 \item We aggregate the image feature across all image patches using the mean and the median.
\end{enumerate}
In detail, for image $i$, patch $j$, I obtain the $k$th raw patch feature as:

$$r_{ijk} =  InceptioNet(x_{ij})_k$$

and, when using the mean aggregation, the final image level features is defined as:

$$f_{ik} = \frac{1}{J}\sum_j r_{ijk}$$

where $J$ is the total number of patches lying within a tissue boundary.

\subsection{Associating features to RNA}

\subsubsection{RNA expression data}

The RNA expression data was download from the GTEx portal and are recorded in log RPKM values.

\subsubsection{Association tests}

To investigate strong drivers of variation between the the expression data and the image features, we performed Pearson Correlation tests between the principal components describing the 95\% of the variation in the image features and expression respectively. To investigate individual transcript-feature relationships, using the method described in the previous section, we generated sample level features in Lung tissue, for a patch-size of 256x256 pixels with the mean as the aggregation method. We selected the top 500 varying features across all patch sizes, and selected the top 2000 varying transcripts which had mean expression greater than $1$. Figure \ref{fig:expression_means_and_stds} displays where the expression cutoff fall on the histograms of expression mean standard deviations respectively. We investigate the Pearson Correlation tests for each of these pairs or transcript and features (500x2000 in total). These correlations are reported together with a p-value representing the probability that the R score was found by chance.

%\subsection{Accounting for technical variation}
%
%To account for technical variation that was apparent in the datasets after the initial exploration, we performed the following steps. We look for the associations with the residuals after regressing the effect of including the first $k$ expression principle components.
%
%We first train the linear model:
%    
%$$Y = X\beta + \epsilon$$
%
%with $Y$ as one of our collection of image fatures and $X$ set as the first $k$ PCs. We then consider the residuals:
%
%$$Y' = Y - X\beta $$
%
%and then we train another linear model:
%
%$$Y' = X\beta' + \epsilon'$$
%
%\begin{wrapfigure}{l}{0.35\textwidth}
%\figuretitle{Classifier validation accuracy across patch-size}
%\includegraphics[width=1\linewidth]{/Classifier/validation_accuracy_across_patchsize}
%\caption{This figure demonstrates the performance of our convolutional neural network in correctly classifying square tissue patches at different patch sizes. We see that at smaller resolutions, validation accuracy decreases but still achieves 81\% accuracy at patchsize 128x128.}
%\label{fig:validation_accuracy_across_patchsize}
%\end{wrapfigure}


\section{Results 1000 words}

\subsection{Tissue classification}

We compared the validation accuracy of using different patch sizes (Figure \ref{fig:validation_accuracy_across_patchsize}). We achieved 81\% accuracy on held-out test data when using $128x128$ patches in training and classification, and 94\% when using $4096x4096$ image patches, demonstrating that neural networks trained on general tasks can be re-purposed for accurate biological image analysis. Despite the high levels of class ambiguity present at small pixel resolutions, the model is still able to achieve high accuracy. For larger patch sizes, global level features like average pixel intensity and tissue edge boundary seem to give accurate features to determine tissue class.



\subsection{Latent Image Feature Association}

\subsubsection{Exploratory Data analysis}

There was also a large amount of heterogeneity in the aggregated image features generated from the histology images across samples (Figure \ref{fig:aggregated_features_across_samples}). There exists variation across sample when using both raw and retrained Inceptionet as feature extractors, but that there exists a greater number of active features with retrained Inceptionet. This is characterised by more horizontal red lines. We ask what drives the variation in later sections.
 
 We visualized the features that were generated from each patch to assess which aggregation method would be most appropriate. Figure \ref{fig:features_across_patches} demonstrates the image feature activations for each patch for two different Lung tissues, with GTEx IDs GTEX-117YW-0526 and GTEX-117YX-1326. Although there exists variation in individual features across patches in a tissue, there are strong drivers that are image specific, and different between each image. Furthermore, we see that although there is variation across patches for specific features, there exists global levels of activation present in all patches in the image, characteristic of an image level feature, rather than just a patch level features. These are indicated by faint horizontal bands, indicating consistent activation across patches for individual features. As such, we concluded that it would be reasonable to use and compare the median and mean activate across all patches in an image as the aggregation method.


How much redundancy exists in these features? Do they capture independent visual characteristics? We consider image features generated from retrained Inceptionet features, mean aggregated at a patch-size of 256. We cross-correlate these features and perform hierarchical clustering understand their orthogonality and frequency of co-occurrence (Figure \ref{fig:feature_crosscorrelation}). Of the 1024 features, 380 were inactive across all samples. Therefore we excluded these and proceeded using only the remaining 644. We see a large group of co-correlated features, but other groups along the diagonal that are independent.

\subsubsection{Feature Associations}

To understand whether drivers of variation in expression are related to drivers of variation in the image features, we took the first 10 expression principle components and the first 20 image feature principle components. We performed a pairwise correlation of these major axes of variation (Figure \ref{fig:expression_PCs_vs_image_feature_PCs}). In the top left hand corner, we see that there exists a strong relationship between expression principle component $1$ and each of the image feature principle components $1$, $3$ and $6$. These have R score and p-values of $(-0.55, 4^{-23})$, $(0.35, 4^{-9})$, $(0.33, 2^{-8})$ respectively. Furthermore, we see a relationship between image feature PC5 and expression PC2, image feature PC2 and expression PC6. These have R score and p-values of $(0.27, 9^{-6})$, $(-0.25, 3^{-5}) $ respectively.


It has been reported that technical factors drive variation in expression \cite{complex-sources-of-variation}, and so we investigated the source of this variation within samples. The GTEx dataset records conditions under which data was collected. For example, the total Ischemic time of a sample (SMTSISCH), RNA degradation number (SMRIN) and the code for the center where the sample was collected (SMCENTER). We collected the values of 51 technical features and correlate them with both the image feature PCs and the expression PCs. (Figure \ref{fig:expression_PCs_vs_TFs} and Figure \ref{fig:image_feature_PCs_vs_TFs}). We see that expression PC1 predominantly captures technical correlation, and is strongly related to the technical factors (Figure \ref{fig:expression_PCs_vs_TFs}). The top $5$ ordered correlations between the expression PCs and technical factors all involve PC1. We see that the same technical features correlate with image feature 1 (Figure \ref{fig:image_feature_PCs_vs_TFs}), albeit in a different order of p-values.

\subsubsection{Quantifying individual significant associations}

We wanted to investigate whether patch size influenced the core number of associations found between the generated image features and expression. To do this we calculated Pearson Correlation coefficients between the top $500$ varying features across all patch sizes, and the top $2000$ varying transcripts that had mean expression greater than $1$. We filter both the image features and transcripts because most transcripts do not vary across samples and most image features are inactive across samples. 

Strikingly, we observe an interesting observation that across $3$ different false discovery thresholds there appears to be an optimum patch-size at $256$x$256$ pixels in order to find Bonferroni significant associations between the aggregated image features and individual transcripts (Figure \ref{fig:expression_cutoff_and_raw_significant_associations}). This is despite the fact that the network was originally retrained to classify size $128$x$128$ pixel patches. The number of associations steadily decreases up to a patch size of $4096$, where the number of associations drops to $0$.

\subsubsection{Comparing Raw vs Retrained InceptioNet}

We assessed the degree to which the training process taught the network to detect tissue specific image features (Figure \ref{fig:associations_raw_vs_retrained}). In the same fashion as before, we extracted aggregated image features using raw InceptioNet. This network had been trained to differentiate natural images, but was not exposed to patches from histopathology tissue images. The final layer representation of raw Inceptionet was a vector of length $2048$, therefore we filtered two sets of aggregated image features across all lung samples to include only the top $500$ varying features across from the retrained and raw network respectively. This was an important step to ensure that we chose the same number of features for both cases. We quantified the number of Bonferroni significant associations across all patches sizes, and at $3$ different false discovery rates. We notice that there are significantly more associations found at a patch size of $256$ in retrained InceptioNet compared to that of raw InceptioNet. However for larger patch sizes, raw InceptioNet generates a greater number of significant associations, achieving a smaller maximum at patch size $512$.

\subsubsection{Comparing Aggregation methods}
We compare the aggregation methods, mean and median, used to combine features generated from individual patches across the extent of the tissue (Figure \ref{fig:associations_mean_vs_median}). We compare the Bonferonni significant associations between image features generated from Lung, at patch-size 256 at three different false discovery rates: $0.01$, $0.0001$ and $0.000001$. Across each FDR the mean aggregate rate generates the greatest number of Bonferroni significant associations. This is true for all patch sizes considered, therefore we concluded that this was the best aggregation method out of the two options.

It might be that single features are strongly associated with the expression of very many genes. This is likely because gene expression is highly correlated amongst groups of genes with a similar biological function. Moreover, the extracted features demonstrate a high degree of correlation (Figure \ref{fig:feature_crosscorrelation}).



\subsubsection{Accounting for covarying transcripts and covarying features}

We count the number of features with Bonferroni significant associations to at least one transcript (Figure \ref{fig:features_with_significant_transcripts}). This gives us an indication of how active an image feature is at a particular patch size. We observe  a similar behaviour to when counting the total numbers of associations. At a patch size of 256 we see the greatest number of features with at least $1$ association to a transcript across all FDR thresholds, decreasing consistently for patch sizes greater than 256x256.


In addition, we instead count the number of transcripts with Bonferroni significant associations to at least 1 feature \ref{fig:transcripts_with_significant_features}. This indicates how many transcripts result in a visual component that is picked up by the generated to image features for each patch size. Unlike before, we observe that the most number of associations is found at patch-size 512, as well as observed a large number of associations at a patch size of 256.


 
%We assessed that correcting for known variation regressing out the technical variation and 2 PCs resulting in appropriate levels of inflation. In Figure \ref{fig:corrected_scatterplots} we display the top 10 associations correcting for technical factors + 2 PCs for lung with patch-size of 256, we found the levels of expression from transcripts with gene IDs AGER, FCN3, and EPAS1 show a strong relation to the aggregated levels of many image features. It has not yet been reported in the literature that the levels of these genes give a visual effect. These transcripts did not display any correlation to known technical factors.


\subsubsection{Feature Interpretation}

We investigate which image features are associated to these 51 technical factors. We see that image feature $796$ is strongly associated to Ischemic time (SMTSISCH) (R -0.55 pvalue 2e-19) (Figure \ref{fig:image_feature_796_vs_SMTSISCH}).

We also investigate what this feature represents (Figure \ref{fig:top5_bottom5_feature796}) by inspecting the 5 samples that maximally activate and deactivate of feature $796$ respectively. We see a clear difference in the colour of the top $5$ and bottom 5 samples. This colour change is strongly related to Ischemic time.

\begin{figure}[H]
\begin{subfigure}{0.7\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{/FeatureExploration/top5_bottom5_feature796}
    \caption{Image feature 796 appears to capture global tissue colour.}
    \label{fig:top5_bottom5_feature796}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{/RawFeatureAssociations/image_feature_796_vs_SMTSISCH}
    \caption{Image feature 796 strongly predicts Ischemic time.}
    \label{fig:image_feature_796_vs_SMTSISCH}
 \end{subfigure}
 \caption{ }
\end{figure}



\section{Discussion 900 words}


\subsection{Tissue Classification}


%Conclusions
We have successfully demonstrated that trained convolutional neural networks can be used to classify tissue type from individual square patches. We conclude that accuracy increases with increasing patch-size, suggesting that global level features like average pixel intensity and tissue edge boundary, features that are available at larger scales, seem to give accurate features to determine tissue class.

%Implications
Although an easy task to a radiologist, it was previously unknown how difficult this task was at smaller resolutions. This work demonstrates that at resolutions as large as 128x128 pixels, there is some ambiguity in predicting tissue class from image data. This is interesting because we can begin to question which parts of tissues are tissue specific, the proportion of cell types that are shared between tissue types, and how more or less similar certain tissues in the body are.

%Limitations
In this work we consider only 10 tissue types, of very different origin. An extension in this direction would be to include tissue patches from all available tissue classes. Furthermore, the smallest patch-size we investigate is $128$x$128$. It would be interesting to investigate questions such as: the smallest possible patch-size where the validation accuracy is greater than 50\%.




\subsection{Latent feature associations}

%Conclusions
Visual characteristics of the images are thought to be driven by levels of bulk RNA expression. We hypothesized that this relationship would be conserved when using the retrained neural network as a feature extractor, and that there would be a quantifiable relationship between these extracted features and transcript levels from bulk RNA expression data. We have shown this to be the case. Broad drivers of variation are shared between the expression datasets and the image features we were generated. Beginning with an Inceptionet neural network containing weights that enable it to classify every day objects, we have demonstrated that we find the greatest number of associations after retraining the neural network to classify tissue type. The maximum is achieved at a patch-size of $256$x$256$, where out of $10$ million association tests as using a Bonferroni significance threshold with $\alpha = 0.0001$ we find twice as many significant associations after retraining the network: $31564$ compared with $66624$ after retraining. Strikingly, for larger patches the trend reverses. For a patch size of $512$x$512$ we find half as many significant associations after retraining: $42235$ comparing with $24721$ after retraining. The reasons for this observation are likely because the network was retrained to classify tissues type at a patch-size of $128$x$128$, thus focussing the network to be sensitive to patches at these smaller resolutions. This is supported by the observation that the number of significant associations for very large patch-sizes drops sharply after retraining, dropping to $60$ significant associations at a patch-size of $4096$x$4096$. One image feature in particular, number 796, has a clear interpretation - it describes the global colour of the tissue. This feature has a strong relationship to Ischemic time and expression PC $1$. We conclude from this that the time between death and tissue harvesting not only is a strong driver of variation in RNA expression in the GTEx dataset but that it causes a strong visual effect. This effect should be accounted for and normalised for any future work involving the GTEx image data. 

%Implications
Out of the patch-sizes we have chosen, it is tempting to conclude that the optimum resolution to understand the relationship between RNA expression and image data is at a patch-size of $256$. However, after considering just number of transcripts that are significant to a given feature (Figure \ref{fig:transcripts_with_significant_features}) we see that the story is not so simple. At a patch-size of $256$x$256$ and $512$x$512$, after retraining the network we see that approximately the same number of transcripts are significant to at least one image feature. For $256$x$256$ patches, we find 1960 associations and for $512$x$512$ patches we find 1915. This implies that approximately the same number of transcripts actually have a relationship with the image features. Since there are approximately $3$ times as many associations found after retraining at a patch-size of $256$x$256$ compared to $512$x$512$, ($66624$ compared with $24721$) and that there are approximately the same number of transcripts. This implies that $3$ times as many image features are active at a patch-size of $256$x$256$ but result in an equivalent number of interesting associations. We recommend that both patch-sizes should be investigated in future studies of a similar nature, and suggest that patch-sizes smaller than this are too small.

This work demonstrates that it is possible to gain information about gene expression using images alone. We find that variation was largely driven by technical factors, including Ischemic time, RNA degradation, and centre at which the sample was collected. 

%Limitations
Many square patch sizes fitted inside the boundary of the tissue slice, and the genetic data available only was representative of the entire image. As such, we needed a method to aggregate the features generated from multiple square patches of the same size, from patches located at different areas of the image into a single feature representative of the entire image. We have only considered two such aggregation methods when many more could have been considered. For example, looking at the maximum activation, or by considering the 90\textsuperscript{th} percentile might be a more appropriate scheme.

Furthermore, using the final layer feature of the neural network compresses entire patches into a single vector of numbers. As such, all spatial structure is lost, and the features are difficult to interpret. We tested methods to investigate the interpretation of these features, such as saliency maps and activation maximisation, but found that neither method returned encouraging results.

\section{Future Work 1000 words}

%\begin{wrapfigure}{l}{0.25\textwidth}
%    \centering
%    \includegraphics[width=0.25\textwidth]{/FCN/FCNhiddenlayer}
%    \caption{Results from training a FCN autoencoder  at a patch-size of 256. The first row demonstrates the raw tissue patches, and the second row displays the decoded patches from the network.}
%    \label{fig:FCNhiddenlayer}
%\end{wrapfigure}


\subsection{Short term}

%Corrected p-value associations
Given that this report has identified that the strongest drivers of variation between image feature and expression are technical factors, the next step in the analysis is to account for this technical variation in a linear mixed model (LMM) and use this to focus on transcripts with relationships to image features outside of known technical variation. These relationships are more likely to be underpinned biologically and more interesting from a biological perspective. Once these corrected associations are characterised, I will perform Gene Ontology analysis to understand the known biology of these transcripts, and hypothesis as to why they are predictable from the images.

%Genotype
Since we were focussed on correcting for technical factors in order to find and interpret associations independent of these, we have neglected searching for genotype associations. This is because we hypothesised that image features driven by technical factors would drown out the association signal to the genotype, which we already anticipate to be underpowered. A next step would be to investigate the relationship to the genotype of image features that show a relationship to transcripts after correcting for technical factors. I will  search for significant associations within a 1kb window of genes which produce transcripts that have significant associations to image features. This will test the hypothesis that image features have a genetic basis.

%Mendelian randomisation
Once genotype relationships have been identified, I will perform Mendelian Randomization \cite{mendelian-randomization} analysis to ascertain whether image features are mediated through expression, or via an independent mechanism.

%Deep Learn review
In a different piece of work that I also completing in the short term: I am co-auther on a review paper titled "Deep Learning in Computational Biology" which has been accepted with minor revisions into the journal "Emerging Topics in Life Sciences". This will require minor further editting along with my co-authors.


\subsection{Medium term}

%Tissue classification paper
I would like to draft the results of the tissue classification part of this report into a short paper. It would be informative to know at which resolution tissue types become indistinguishable. It is not known whether features at very small resolutions, like colour, can be discriminative. This will answer questions of how different tissues are visually, and could potentially be linked to stages of embryonic development. As an unstudied question this work could answer the resolution at which tissue slices become unique. Furthermore, it flips the conventional use case of neural networks in classification. Instead of testing whether a neural network is able to find discriminative features for a particular task, it assumes that such features would be discoverable by a neural network, if indeed they exist. We aim to investigate whether these feature exist at all, or at least that if they do exist, they cannot be modelling by a neural network.

%Convolutional Architecture

The deep aggregated features that I generated are not easily interpretable. The most interpretable feature only managed to capture the global change of colour in the tissue slice. The reason for this was because the current network architecture collapsed all convolutional layers into a single dense layer, destroying all spatial relationships between the pixels in the input patch. Modern day autoencoders used in computer vision employ Fully Connected Networks (FCNs) \cite{fully-convolutional-networks} architectures for encoding images into compressed representations that are rectangular in shape, and that conserve these spatial relationships. Recently, researchers have used these architectures to perform unsupervised nucleus detection \cite{sparse-autoencoders}  and segmentation of epithelial and stromal regions \cite{segmenting-classifying-epithelial}. Applying these methods to the GTEx data has the potential to give accurate approximations to the proportion of different cell types e.g. lymphocytes in a histopathology tissue slice. Using this, we can whether a relationship exists between cell-type proportion and specific gene transcripts or genotypes. If these features were representative of cell type composition across a tissue slice, it is very likely be related to levels of individual transcripts. This is because the RNA expression data is generated in bulk from a sample taken from the tissue slice, and gene expression is strongly determined by cell-type. It is highly plausible that this would also be linked between to genotype. The benefit of this approach is that these representations actually have a clear interpretation as the proportion of cell-type composition.


\subsection{Long term}

%Latent Factor association paper
In the longer term, having performed the corrected p-value analysis, genotype association analysis, and Mendelian randomisation analysis, I aim to write up the Latent Factor association work into a paper. This will be one of the first example of employing methods from deep learning on images with the purpose of relating them to genetic data. I know of a competing group that is employing similar methods, and so will aim to write this work up as soon as the result become available.

%Investigate GANs VAEs
I would like to pursue other classes of neural network models in the recent deep learning literature that have given promising results. Generative Adverserial Networks (GANs) \cite{generative-adverserial-networks} and Variational Autoencoders \cite{variational-autoencoders} are able to define latent image representations, and creating realistic images with them as input. For example, recent work has demonstrated that it is possible to generate realistic looking images of protein abundance classes in high-content screening \cite{gans-biological-image-synthesis}.

%similar analysis on different datasets (HIPSCI).
I have access to other datasets that contain image data as well as the corresponding expression and genotype data, for example the Human Induced Pluripotent Stem Cells Initiative (HIPSCI) \cite{hipsci-project} contains annotated images of single cells. It could be possible and fruitful to employ the methods developed in this report on this HIPSCI data.

\subsection{Gantt chart}



\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{/Gantt/Gantt}
\caption{Gantt chart}
\label{fig:gantt}
\end{figure}


\section{Figures}

\begin{figure}[H]
\figuretitle{Feature variation across patches}
\centering
\includegraphics[width=1\textwidth]{/FeatureExploration/features_across_patches}
\caption{Variation in 50 aggregated image features across 100 patches from two different Lung samples. Thumbnails of the images are displayed on the right hand side. Despite visible feature variation across patches, there is evidence of image feature activity that is consistent across patches, indicated by the faint horizontal lines. This motivates aggregating across patches, using either the mean or the median.}
\label{fig:features_across_patches}
\end{figure}

\begin{figure}[H]

\begin{subfigure}{0.45\textwidth}
\figuretitle{Aggregated Features across Lung Samples}
\includegraphics[width=1\linewidth]{/FeatureExploration/aggregated_features_across_samples} 
\caption{50 aggregated image feature values across 100 lung image samples. The image features are mean aggregated and generated from the retrained Inceptionet model at a patch size of $256$ for retrained Inceptionet \textcolor{ao(english)}{A} , and raw Inceptionet \textcolor{ao(english)}{B}. More features activate in retrained Inceptionet.}
\label{fig:aggregated_features_across_samples}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\textwidth}
\figuretitle{Feature Cross-Correlation}
\includegraphics[width=1\linewidth]{/FeatureExploration/feature_crosscorrelation}
\caption{Hierachical clustering performed on aggregated image features with non-zero standard deviation.}
\label{fig:feature_crosscorrelation}
\end{subfigure}

\caption{ }

\end{figure}



\begin{figure}[H]

\begin{subfigure}{0.45\textwidth}
\figuretitle{Expression PCs vs Technical Factors}
\includegraphics[width=1\linewidth]{/PCAFeatureAssociations/expression_PCs_vs_TFs} 
\caption{A strong relationship exists between expression PC1 and 5 technical factors. We display \textcolor{ao(english)}{A} Pearson correlation coefficient and \textcolor{ao(english)}{B} -log10 pvalues, between 51 technical factors (y-axis) and the first 10 expression PCs (x-axis). Scatterplots of expression PC1 and \textcolor{ao(english)}{C} Ischemic time,  \textcolor{ao(english)}{D} Intronic mapping rate, \textcolor{ao(english)}{E} Exonic mapping rate, \textcolor{ao(english)}{F} RNA degradation, \textcolor{ao(english)}{G} Autolysis score.}
\label{fig:expression_PCs_vs_TFs}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\textwidth}
\figuretitle{Image Feature PCs vs Technical Factors}
\includegraphics[width=1\linewidth]{/PCAFeatureAssociations/image_feature_PCs_vs_TFs}
\caption{A strong relationship exists between expression PC1 and 5 technical factors. We display \textcolor{ao(english)}{A} Pearson correlation coefficient and \textcolor{ao(english)}{B} -log10 pvalues, between 51 technical factors (y-axis) and the first 10 image feature PCs (x-axis). Scatterplots of expression PC1 and \textcolor{ao(english)}{C}RNA degradation,  \textcolor{ao(english)}{D} Ischemic time, \textcolor{ao(english)}{E} Autolysis score, \textcolor{ao(english)}{F} Intronic mapping rate., \textcolor{ao(english)}{G} Exonic mapping rate.}
\label{fig:image_feature_PCs_vs_TFs}
\end{subfigure}

\caption{Major sources of variation in image and gene expression data are not independent}
\label{fig:technical_factors_drive_variation}
\end{figure}

\begin{figure}[H]
\centering
\figuretitle{Expression PCs vs Image Features PCs}
\includegraphics[width=1\textwidth]{/PCAFeatureAssociations/expression_PCs_vs_image_feature_PCs}
\caption{Major shared variation is between PC1 of both the image features and expression. We display \textcolor{ao(english)}{A} Pairwise pearsonr correlation and \textcolor{ao(english)}{B} -log10 pvalues between the top $20$ image feature PCs and the top $10$ expression PCs. \textcolor{ao(english)}{C-G} Scatterplot of top 5 (ordered by R\textsuperscript{2} ) associations between expression and image PCs. }
\label{fig:expression_PCs_vs_image_feature_PCs}
\end{figure}

\begin{figure}[H]

\begin{subfigure}{0.45\textwidth}
\figuretitle{Filtering expression matrix}
\includegraphics[width=1\linewidth]{/FeatureExploration/expression_means_and_stds}
\caption{Summary statistics of expression levels used for gene filtering \textcolor{ao(english)}{A} Frequency (y-axis) of mean transcript expression across samples (x-axis). Red vertical line corresponds to x=1, and was used as a minimum expression level cutoff for inclusion in the analysed dataset. \textcolor{ao(english)}{B} Frequency (y-axis) of the standard deviation of transcript expression across samples (x-axis) that have mean expression greater than 1. Samples to the right of the vertical red line correspond to the 2000 most varying transcripts.}
\label{fig:expression_means_and_stds}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\textwidth}
\figuretitle{Significant associations varying FDR}
\includegraphics[width=1\linewidth]{/RawFeatureAssociations/raw_associations_across_patchsizes}
\caption{Relationship between raw number of Bonferroni significant associations for 3 different FDRs: $0.01$, $0.0001$ and $0.000001$. We see that at a patch size of 256x256 we find the greatest number of associations across all FDRs.}
\label{fig:associations_raw_vs_retrained}
\end{subfigure}

\caption{The left figure illustrates the expression data cutoffs. The right hand figure displays the number of significant associations between levels of individual transcripts across samples, and the aggregated image features. The image features are from Lung images, mean aggregated, and generated from retrained Inception at a patch size of $256$.}
\label{fig:expression_cutoff_and_raw_significant_associations}
\end{figure}

\begin{figure}[!htb]
\minipage{0.23\textwidth}
\figuretitle{Significant associations varying model}
\includegraphics[width=1\linewidth]{/RawFeatureAssociations/associations_raw_vs_retrained}
  \caption{Comparing the significant association count (y-axis) across patch size (x-axis) between raw and retrained Inceptionet. (Bonf $\alpha$ = 0.0001)}\label{fig:associations_raw_vs_retrained}
\endminipage\hfill
\minipage{0.23\textwidth}
\figuretitle{Significant associations varying Aggregation}
\includegraphics[width=1\linewidth]{/RawFeatureAssociations/associations_mean_vs_median}
  \caption{Comparing the significant association count (y-axis) across patch size (x-axis) between mean and median aggregation. (Bonf $\alpha$ = 0.0001)}\label{fig:associations_mean_vs_median}
\endminipage\hfill
\minipage{0.23\textwidth}%
\figuretitle{Features with significant transcripts}
\includegraphics[width=1\linewidth]{/RawFeatureAssociations/features_with_significant_transcripts}
  \caption{Counting the number of features that have a significant association to at least $1$ transcript. (Bonf $\alpha$ = 0.0001)}\label{fig:features_with_significant_transcripts}
\endminipage\hfill
\minipage{0.23\textwidth}%
\figuretitle{Transcripts with significant features}
\includegraphics[width=1\linewidth]{/RawFeatureAssociations/transcripts_with_significant_features}
  \caption{Counting the number of transcripts that have a significant association to at least $1$ feature. (Bonf $\alpha$ = 0.0001)}\label{fig:transcripts_with_significant_features}
\endminipage

\end{figure}

%\begin{figure}[!htb]
%\minipage{0.32\textwidth}
%\figuretitle{Raw p-values with calibrations}
%  \includegraphics[width=\linewidth]{/InflationPvalues/raw_pvalues}
%  \caption{Figure displays a QQplot of the raw p-values generated }\label{fig:raw_pvalues}
%\endminipage\hfill
%\minipage{0.32\textwidth}
%\figuretitle{Correcting p-values}
%  \includegraphics[width=\linewidth]{/InflationPvalues/raw_vs_corrected_pvalues}
%  \caption{Raw p-values vs corrected p-values with 1 expression PC}\label{fig:raw_vs_corrected_pvalues}
%\endminipage\hfill
%\minipage{0.32\textwidth}%
%\figuretitle{Comparing the number of correction PCs }
%  \includegraphics[width=\linewidth]{/InflationPvalues/corrected_pvalues}
%  \caption{Correcting the p-values with different numbers of expression PCs}\label{fig:corrected_pvalues}
%\endminipage
%\end{figure}




%\begin{figure}[H]
%
%\begin{subfigure}{0.45\textwidth}
%    \figuretitle{Comparing mean and median aggregation}
%    \includegraphics[width=1\linewidth]{/RawFeatureAssociations/associations_mean_vs_median}
%    \caption{A graph showing the relationship between raw number of Bonferroni significant associations for 3 different FDRs: $1-e2$, $1-e4$ and $1-e6$. We see that at a patch size of 256x256 we find the greater number of associations across all FDRs.}
%    \end{subfigure}
%\hfill
%
%\begin{subfigure}{0.45\textwidth}
%\figuretitle{Comparing raw vs retrained Inceptionet}
%\includegraphics[width=1\linewidth]{/RawFeatureAssociations/associations_raw_vs_retrained}
%\caption{A graph showing the relationship between raw number of Bonferroni significant associations for $3$ different FDRs: $1-e2$, $1-e4$ and $1-e6$. We see that at a patch size of $256x256$ we find the greater number of associations across all FDRs.}
%\end{subfigure}
%
%\caption{Plot a) demonstrates that aggregating via the mean preserves a greater number of Bonferroni significant associations. Figure b) demonstrates that the greatest number of associations are found with retrained Inceptionet at a patch-size of 256.}
%\label{fig:comparing_aggregation_and_model}
%\end{figure}
%



%\begin{figure}[H]
%\centering
%\includegraphics[width=0.7\textwidth]{top5_bottom5_samples351}
%\caption{The top row displays the top 5 samples for aggregated feature 351, the bottom row displays the bottom 5 samples. Notice the stark difference in colour.}
%\label{fig:top5_bottom5_samples351}
%\end{figure}

%\begin{figure}[H]
%\centering
%\includegraphics[width=0.7\textwidth]{top5_bottom5_samples797}
%\caption{The top row displays the top 5 samples for aggregated feature 351, the bottom row displays the bottom 5 samples. Notice the stark difference in porosity.}
%\label{fig:top5_bottom5_samples797}
%\end{figure}


%\begin{figure}[H]
%\centering
%\includegraphics[width=0.7\textwidth]{example_inflation_lung_256_mean}
%\caption{}
%\label{fig:example_inflation_lung_256_mean}
%\end{figure}


%\begin{figure}[H]
%\centering
%\includegraphics[width=0.7\textwidth]{corrected_pvalues_qqplot}
%\caption{}
%\label{fig:corrected_pvalues_qqplot}
%\end{figure}

%\begin{figure}[H]
%\centering
%\includegraphics[width=0.7\textwidth]{corrected_scatterplots}
%\caption{}
%\label{fig:corrected_scatterplots}
%\end{figure}

\section{Tables}

\begin{table}[H]
\caption{Top 5 R\textsuperscript{2} associations between Expression PCs and technical factors}
\label{tab:expression_pcs_and_tfs}       % Give a unique label
%
% Follow this input for your own table layout

\begin{tabular}{p{1cm}p{2.4cm}p{11cm}p{1cm}p{1cm}}
\hline\noalign{\smallskip}
 PC & Technical factor & Description & R score & p-value  \\
\noalign{\smallskip}\svhline\noalign{\smallskip}
1 & SMTSISCH & Total Ischemic time for a sample in 4 hour intervals & 0.78 & 3e-48\\
1 & SMNTRNRT & Intronic Rate: The fraction of reads that map within introns & 0.77 & 4e-45\\
1 & SMEXNCRT & Exonic Rate: The fraction of reads that map within exons & -0.73 & 3e-39 \\
1 & SMRIN & RIN Number (RNA degradation) & -0.61 & 2e-24 \\
1 & SMATSSCR & Autolysis Score & 0.46 & 3e-13 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}
% $^a$ Table foot note (with superscript)
\end{table}

\begin{table}[H]
\caption{Top 5 R\textsuperscript{2} associations between Image Feature PCs and technical factors}
\label{tab:image_feature_pcs_and_tfs}       % Give a unique label

\begin{tabular}{p{1cm}p{2.4cm}p{11cm}p{1cm}p{1cm}}
\hline\noalign{\smallskip}
PC & Technical factor & Description & R score & p-value  \\
\noalign{\smallskip}\svhline\noalign{\smallskip}
1 & SMRIN & RIN Number (RNA degradation) & 0.49 & 2e-15\\
1 & SMTSISCH & Total Ischemic time for a sample in 4 hour intervals, & 0.77 & 4e-13\\
1 & SMATSSCR & Autolysis Score & -0.42 & 4e-11 \\
1 & SMNTRNRT & Intronic Rate: The fraction of reads that map within introns & -0.41 & 8e-11 \\
1 & SMEXNCRT & Exonic Rate: The fraction of reads that map within exons & 0.37 & 5e-09 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}
% $^a$ Table foot note (with superscript)
\end{table}



\input{referenc}
\end{document}
